{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Convolution1D, Flatten, Dropout, Dense, MaxPool1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./train_twitter.csv')\n",
    "df_test = pd.read_csv('./test_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
       "1  31964   @user #white #supremacists want everyone to s...\n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "3  31966  is the hp and the cursed child book up for res...\n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tidy_tweet'] = np.vectorize(remove_pattern)(df['tweet'], \"@[\\w]*\") # remove all @user\n",
    "df_test['tidy_tweet'] = np.vectorize(remove_pattern)(df_test['tweet'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters, numbers, punctuations\n",
    "df['tidy_tweet'] = df['tidy_tweet'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "df_test['tidy_tweet'] = df_test['tidy_tweet'].str.replace(\"[^a-zA-Z]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep tweets having more than 3 words in train set\n",
    "df['tidy_tweet'] = df['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>when father dysfunctional selfish drags kids i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks lyft credit cause they offer wheelchair...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>model love take with time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
       "      <td>huge fare talking before they leave chaos disp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
       "      <td>camping tomorrow danny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>the next school year is the year for exams.ð...</td>\n",
       "      <td>next school year year exams think about that s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
       "      <td>love land allin cavs champions cleveland cleve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n",
       "      <td>welcome here</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1      0   @user when a father is dysfunctional and is s...   \n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3      0                                bihday your majesty   \n",
       "3   4      0  #model   i love u take with u all the time in ...   \n",
       "4   5      0             factsguide: society now    #motivation   \n",
       "5   6      0  [2/2] huge fan fare and big talking before the...   \n",
       "6   7      0   @user camping tomorrow @user @user @user @use...   \n",
       "7   8      0  the next school year is the year for exams.ð...   \n",
       "8   9      0  we won!!! love the land!!! #allin #cavs #champ...   \n",
       "9  10      0   @user @user welcome here !  i'm   it's so #gr...   \n",
       "\n",
       "                                          tidy_tweet  \n",
       "0  when father dysfunctional selfish drags kids i...  \n",
       "1  thanks lyft credit cause they offer wheelchair...  \n",
       "2                                bihday your majesty  \n",
       "3                          model love take with time  \n",
       "4                      factsguide society motivation  \n",
       "5  huge fare talking before they leave chaos disp...  \n",
       "6                             camping tomorrow danny  \n",
       "7  next school year year exams think about that s...  \n",
       "8  love land allin cavs champions cleveland cleve...  \n",
       "9                                       welcome here  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>studiolife  aislife  requires  passion  dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>white  supremacists want everyone to see th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "      <td>safe ways to heal your  acne       altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "      <td>rd  bihday to my amazing  hilarious  nephew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31968</td>\n",
       "      <td>choose to be   :) #momtips</td>\n",
       "      <td>choose to be       momtips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31969</td>\n",
       "      <td>something inside me dies ð¦ð¿â¨  eyes nes...</td>\n",
       "      <td>something inside me dies              eyes nes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31970</td>\n",
       "      <td>#finished#tattoo#inked#ink#loveitâ¤ï¸ #â¤ï¸...</td>\n",
       "      <td>finished tattoo inked ink loveit             ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31971</td>\n",
       "      <td>@user @user @user i will never understand why...</td>\n",
       "      <td>i will never understand why my dad left me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31972</td>\n",
       "      <td>#delicious   #food #lovelife #capetown mannaep...</td>\n",
       "      <td>delicious    food  lovelife  capetown mannaep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet  \\\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...   \n",
       "1  31964   @user #white #supremacists want everyone to s...   \n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...   \n",
       "3  31966  is the hp and the cursed child book up for res...   \n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew...   \n",
       "5  31968                        choose to be   :) #momtips    \n",
       "6  31969  something inside me dies ð¦ð¿â¨  eyes nes...   \n",
       "7  31970  #finished#tattoo#inked#ink#loveitâ¤ï¸ #â¤ï¸...   \n",
       "8  31971   @user @user @user i will never understand why...   \n",
       "9  31972  #delicious   #food #lovelife #capetown mannaep...   \n",
       "\n",
       "                                          tidy_tweet  \n",
       "0   studiolife  aislife  requires  passion  dedic...  \n",
       "1     white  supremacists want everyone to see th...  \n",
       "2  safe ways to heal your  acne       altwaystohe...  \n",
       "3  is the hp and the cursed child book up for res...  \n",
       "4     rd  bihday to my amazing  hilarious  nephew...  \n",
       "5                        choose to be       momtips   \n",
       "6  something inside me dies              eyes nes...  \n",
       "7   finished tattoo inked ink loveit             ...  \n",
       "8      i will never understand why my dad left me...  \n",
       "9   delicious    food  lovelife  capetown mannaep...  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # tokenizing\n",
    "tokenized_tweet = df['tidy_tweet'].apply(lambda x:  word_tokenize(x))\n",
    "tokenized_tweet_test = df_test['tidy_tweet'].apply(lambda x:  word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stop words\n",
    "stop = set(stopwords.words('english'))\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x : [i for i in x if i not in stop])\n",
    "tokenized_tweet_test = tokenized_tweet_test.apply(lambda x : [i for i in x if i not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [father, dysfunctional, selfish, drags, kids, ...\n",
       "1    [thanks, lyft, credit, cause, offer, wheelchai...\n",
       "2                                    [bihday, majesty]\n",
       "3                            [model, love, take, time]\n",
       "4                    [factsguide, society, motivation]\n",
       "Name: tidy_tweet, dtype: object"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [father, dysfunctional, selfish, drag, kid, dy...\n",
       "1    [thanks, lyft, credit, cause, offer, wheelchai...\n",
       "2                                    [bihday, majesty]\n",
       "3                            [model, love, take, time]\n",
       "4                    [factsguide, society, motivation]\n",
       "Name: tidy_tweet, dtype: object"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmetizing\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [lmtzr.lemmatize(i) for i in x]) # stemming\n",
    "tokenized_tweet_test = tokenized_tweet_test.apply(lambda x: [lmtzr.lemmatize(i) for i in x])\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [father, dysfunct, selfish, drag, kid, dysfunct]\n",
       "1    [thank, lyft, credit, caus, offer, wheelchair,...\n",
       "2                                    [bihday, majesti]\n",
       "3                            [model, love, take, time]\n",
       "4                          [factsguid, societi, motiv]\n",
       "Name: tidy_tweet, dtype: object"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "tokenized_tweet_test = tokenized_tweet_test.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "tokenized_tweet.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [studiolif, aislif, requir, passion, dedic, wi...\n",
       "1        [white, supremacist, want, everyon, see, new, ...\n",
       "2        [safe, way, heal, acn, altwaystoh, healthi, heal]\n",
       "3        [hp, curs, child, book, reserv, alreadi, ye, h...\n",
       "4        [rd, bihday, amaz, hilari, nephew, eli, ahmir,...\n",
       "5                                          [choos, momtip]\n",
       "6        [someth, insid, dy, eye, ness, smokeyey, tire,...\n",
       "7         [finish, tattoo, ink, ink, loveit, thank, aleee]\n",
       "8        [never, understand, dad, left, young, deep, in...\n",
       "9        [delici, food, lovelif, capetown, mannaepicur,...\n",
       "10       [dayswast, narcosi, infinit, ep, make, awar, g...\n",
       "11       [one, world, greatest, spo, event, leman, team...\n",
       "12                        [half, way, websit, allgoingwel]\n",
       "13       [good, food, good, life, enjoy, call, garlic, ...\n",
       "14       [stand, behind, guncontrolpleas, senselessshoo...\n",
       "15       [ate, ate, ate, jamaisasthi, fish, curri, praw...\n",
       "16             [got, limit, edit, rain, shine, set, today]\n",
       "17       [amp, love, amp, hug, amp, kiss, keep, babi, p...\n",
       "18                [girl, sun, fave, london, unit, kingdom]\n",
       "19       [thought, factori, bbc, neutral, right, wing, ...\n",
       "20       [hey, guy, tommorow, last, day, exam, happi, yay]\n",
       "21       [levyrroni, recuerdo, memori, recuerdo, friend...\n",
       "22            [mind, like, bodi, like, sleepi, stillallin]\n",
       "23                                    [never, entir, life]\n",
       "24       [check, twitterww, trend, trend, worldwid, bst...\n",
       "25       [thought, saw, mermaid, ceege, smcr, inshot, g...\n",
       "26                 [chick, get, fuck, hottest, nake, ladi]\n",
       "27       [happi, bday, luci, xoxo, love, beauti, pizza,...\n",
       "28       [haroldfriday, weekend, fill, sunbeam, everyon...\n",
       "29       [tri, noth, tri, know, love, rd, light, fave, ...\n",
       "                               ...                        \n",
       "17167                [peopl, anyth, fuck, attent, nowaday]\n",
       "17168    [creativ, bubbl, got, burst, look, forward, da...\n",
       "17169    [tomorrow, gon, na, big, day, go, deliv, first...\n",
       "17170                   [thank, babi, giggl, thank, posit]\n",
       "17171                  [model, love, u, take, u, time, ur]\n",
       "17172    [life, u, grow, learn, pple, work, fuck, u, co...\n",
       "17173    [storm, rain, togeth, destroy, town, becam, na...\n",
       "17174    [lovelgq, broken, ep, via, rnb, love, heabroke...\n",
       "17175    [spread, love, hate, prayingfororlando, lovean...\n",
       "17176                                  [racist, pay, ever]\n",
       "17177                         [thank, child, thank, posit]\n",
       "17178    [liverpool, walk, liverpool, starbuck, avidaeb...\n",
       "17179    [bakersfield, rooster, simul, want, climb, vas...\n",
       "17180    [por, sol, instagood, beauti, instadaili, inst...\n",
       "17181    [hell, yeah, great, surpris, present, enjoy, p...\n",
       "17182              [ur, joke, ur, defens, toward, everyth]\n",
       "17183    [enjoy, even, sun, bedroom, cozi, even, homesw...\n",
       "17184    [tonight, pm, gmt, special, earli, play, new, ...\n",
       "17185    [today, good, day, excercis, imreadi, sofucken...\n",
       "17186    [good, night, tea, music, billi, music, tea, m...\n",
       "17187    [love, life, createyourfutur, lifestyl, holida...\n",
       "17188    [black, professor, demon, propos, nazi, style,...\n",
       "17189    [learn, think, posit, posit, instagram, instag...\n",
       "17190    [love, pretti, happi, fresh, teenilici, fixder...\n",
       "17191    [damn, tuff, ruff, muff, techno, citi, ng, web...\n",
       "17192    [thought, factori, left, right, polaris, trump...\n",
       "17193    [feel, like, mermaid, hairflip, neverreadi, fo...\n",
       "17194    [hillari, campaign, today, ohio, omg, amp, use...\n",
       "17195    [happi, work, confer, right, mindset, lead, cu...\n",
       "17196    [song, glad, free, download, shoegaz, newmus, ...\n",
       "Name: tidy_tweet, Length: 17197, dtype: object"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweet_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tidy_tweet'] = tokenized_tweet\n",
    "df_test['tidy_tweet'] = tokenized_tweet_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using all train and test set examples to cover whole vocabulary\n",
    "token.fit_on_texts(pd.concat([df['tidy_tweet'],df_test['tidy_tweet']])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index tokens\n",
    "train_tokens = token.texts_to_sequences(df['tidy_tweet'])\n",
    "test_tokens = token.texts_to_sequences(df_test['tidy_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad each tweet so that all tweets have equal words\n",
    "X_train = pad_sequences(train_tokens, maxlen=30)\n",
    "X_test = pad_sequences(test_tokens, maxlen=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,   11, 5690, 2208, 1776,  124, 5690], dtype=int32)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = len(token.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample train and validation sets\n",
    "X_train1 = X_train[:25000,:] \n",
    "X_val = X_train[25000:,:]\n",
    "\n",
    "y_train1 = y[:25000]\n",
    "y_val = y[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 6962 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 19s 765us/step - loss: 0.3018 - acc: 0.9189 - val_loss: 0.2101 - val_acc: 0.9299\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 17s 687us/step - loss: 0.1296 - acc: 0.9527 - val_loss: 0.1238 - val_acc: 0.9560\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 17s 690us/step - loss: 0.0602 - acc: 0.9802 - val_loss: 0.1294 - val_acc: 0.9576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b198c05c0>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Embedding and LSTM layers\n",
    "embedding_vector_length = 300 \n",
    "model = Sequential() \n",
    "model.add(Embedding(vocab + 1, embedding_vector_length,input_length=30)) \n",
    "model.add(LSTM(100))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train1, y_train1, validation_data=(X_val, y_val), nb_epoch=3, batch_size=512) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1015], dtype=int32)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(model.predict_classes(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6589595375722542"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val, model.predict_classes(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model.predict_classes(X_test)).to_csv('pred_twitter_lstm.csv') # Final predictions on test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
