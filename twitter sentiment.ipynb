{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "# Python program to generate word vectors using Word2Vec \n",
    "# importing all necessary modules \n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from xgboost import XGBClassifier\n",
    "import warnings \n",
    "import nltk  \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import confusion_matrix\n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Convolution1D, Flatten, Dropout, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./train_twitter.csv')\n",
    "df_test = pd.read_csv('./test_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
       "1  31964   @user #white #supremacists want everyone to s...\n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "3  31966  is the hp and the cursed child book up for res...\n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tidy_tweet'] = np.vectorize(remove_pattern)(df['tweet'], \"@[\\w]*\") # remove all @user\n",
    "df_test['tidy_tweet'] = np.vectorize(remove_pattern)(df_test['tweet'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters, numbers, punctuations\n",
    "df['tidy_tweet'] = df['tidy_tweet'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "df_test['tidy_tweet'] = df_test['tidy_tweet'].str.replace(\"[^a-zA-Z]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep tweets having more than 3 words in train set\n",
    "df['tidy_tweet'] = df['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>when father dysfunctional selfish drags kids i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks lyft credit cause they offer wheelchair...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>model love take with time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
       "      <td>huge fare talking before they leave chaos disp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
       "      <td>camping tomorrow danny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>the next school year is the year for exams.ð...</td>\n",
       "      <td>next school year year exams think about that s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
       "      <td>love land allin cavs champions cleveland cleve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n",
       "      <td>welcome here</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1      0   @user when a father is dysfunctional and is s...   \n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3      0                                bihday your majesty   \n",
       "3   4      0  #model   i love u take with u all the time in ...   \n",
       "4   5      0             factsguide: society now    #motivation   \n",
       "5   6      0  [2/2] huge fan fare and big talking before the...   \n",
       "6   7      0   @user camping tomorrow @user @user @user @use...   \n",
       "7   8      0  the next school year is the year for exams.ð...   \n",
       "8   9      0  we won!!! love the land!!! #allin #cavs #champ...   \n",
       "9  10      0   @user @user welcome here !  i'm   it's so #gr...   \n",
       "\n",
       "                                          tidy_tweet  \n",
       "0  when father dysfunctional selfish drags kids i...  \n",
       "1  thanks lyft credit cause they offer wheelchair...  \n",
       "2                                bihday your majesty  \n",
       "3                          model love take with time  \n",
       "4                      factsguide society motivation  \n",
       "5  huge fare talking before they leave chaos disp...  \n",
       "6                             camping tomorrow danny  \n",
       "7  next school year year exams think about that s...  \n",
       "8  love land allin cavs champions cleveland cleve...  \n",
       "9                                       welcome here  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>studiolife  aislife  requires  passion  dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>white  supremacists want everyone to see th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "      <td>safe ways to heal your  acne       altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "      <td>rd  bihday to my amazing  hilarious  nephew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31968</td>\n",
       "      <td>choose to be   :) #momtips</td>\n",
       "      <td>choose to be       momtips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31969</td>\n",
       "      <td>something inside me dies ð¦ð¿â¨  eyes nes...</td>\n",
       "      <td>something inside me dies              eyes nes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31970</td>\n",
       "      <td>#finished#tattoo#inked#ink#loveitâ¤ï¸ #â¤ï¸...</td>\n",
       "      <td>finished tattoo inked ink loveit             ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31971</td>\n",
       "      <td>@user @user @user i will never understand why...</td>\n",
       "      <td>i will never understand why my dad left me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31972</td>\n",
       "      <td>#delicious   #food #lovelife #capetown mannaep...</td>\n",
       "      <td>delicious    food  lovelife  capetown mannaep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet  \\\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...   \n",
       "1  31964   @user #white #supremacists want everyone to s...   \n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...   \n",
       "3  31966  is the hp and the cursed child book up for res...   \n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew...   \n",
       "5  31968                        choose to be   :) #momtips    \n",
       "6  31969  something inside me dies ð¦ð¿â¨  eyes nes...   \n",
       "7  31970  #finished#tattoo#inked#ink#loveitâ¤ï¸ #â¤ï¸...   \n",
       "8  31971   @user @user @user i will never understand why...   \n",
       "9  31972  #delicious   #food #lovelife #capetown mannaep...   \n",
       "\n",
       "                                          tidy_tweet  \n",
       "0   studiolife  aislife  requires  passion  dedic...  \n",
       "1     white  supremacists want everyone to see th...  \n",
       "2  safe ways to heal your  acne       altwaystohe...  \n",
       "3  is the hp and the cursed child book up for res...  \n",
       "4     rd  bihday to my amazing  hilarious  nephew...  \n",
       "5                        choose to be       momtips   \n",
       "6  something inside me dies              eyes nes...  \n",
       "7   finished tattoo inked ink loveit             ...  \n",
       "8      i will never understand why my dad left me...  \n",
       "9   delicious    food  lovelife  capetown mannaep...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " # tokenizing\n",
    "tokenized_tweet = df['tidy_tweet'].apply(lambda x:  word_tokenize(x))\n",
    "tokenized_tweet_test = df_test['tidy_tweet'].apply(lambda x:  word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stop words\n",
    "stop = set(stopwords.words('english'))\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x : [i for i in x if i not in stop])\n",
    "tokenized_tweet_test = tokenized_tweet_test.apply(lambda x : [i for i in x if i not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [father, dysfunctional, selfish, drags, kids, ...\n",
       "1    [thanks, lyft, credit, cause, offer, wheelchai...\n",
       "2                                    [bihday, majesty]\n",
       "3                            [model, love, take, time]\n",
       "4                    [factsguide, society, motivation]\n",
       "Name: tidy_tweet, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [father, dysfunctional, selfish, drag, kid, dy...\n",
       "1    [thanks, lyft, credit, cause, offer, wheelchai...\n",
       "2                                    [bihday, majesty]\n",
       "3                            [model, love, take, time]\n",
       "4                    [factsguide, society, motivation]\n",
       "Name: tidy_tweet, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmetizing\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [lmtzr.lemmatize(i) for i in x]) # stemming\n",
    "tokenized_tweet_test = tokenized_tweet_test.apply(lambda x: [lmtzr.lemmatize(i) for i in x])\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [father, dysfunct, selfish, drag, kid, dysfunct]\n",
       "1    [thank, lyft, credit, caus, offer, wheelchair,...\n",
       "2                                    [bihday, majesti]\n",
       "3                            [model, love, take, time]\n",
       "4                          [factsguid, societi, motiv]\n",
       "Name: tidy_tweet, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "tokenized_tweet_test = tokenized_tweet_test.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "tokenized_tweet.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [studiolif, aislif, requir, passion, dedic, wi...\n",
       "1        [white, supremacist, want, everyon, see, new, ...\n",
       "2        [safe, way, heal, acn, altwaystoh, healthi, heal]\n",
       "3        [hp, curs, child, book, reserv, alreadi, ye, h...\n",
       "4        [rd, bihday, amaz, hilari, nephew, eli, ahmir,...\n",
       "5                                          [choos, momtip]\n",
       "6        [someth, insid, dy, eye, ness, smokeyey, tire,...\n",
       "7         [finish, tattoo, ink, ink, loveit, thank, aleee]\n",
       "8        [never, understand, dad, left, young, deep, in...\n",
       "9        [delici, food, lovelif, capetown, mannaepicur,...\n",
       "10       [dayswast, narcosi, infinit, ep, make, awar, g...\n",
       "11       [one, world, greatest, spo, event, leman, team...\n",
       "12                        [half, way, websit, allgoingwel]\n",
       "13       [good, food, good, life, enjoy, call, garlic, ...\n",
       "14       [stand, behind, guncontrolpleas, senselessshoo...\n",
       "15       [ate, ate, ate, jamaisasthi, fish, curri, praw...\n",
       "16             [got, limit, edit, rain, shine, set, today]\n",
       "17       [amp, love, amp, hug, amp, kiss, keep, babi, p...\n",
       "18                [girl, sun, fave, london, unit, kingdom]\n",
       "19       [thought, factori, bbc, neutral, right, wing, ...\n",
       "20       [hey, guy, tommorow, last, day, exam, happi, yay]\n",
       "21       [levyrroni, recuerdo, memori, recuerdo, friend...\n",
       "22            [mind, like, bodi, like, sleepi, stillallin]\n",
       "23                                    [never, entir, life]\n",
       "24       [check, twitterww, trend, trend, worldwid, bst...\n",
       "25       [thought, saw, mermaid, ceege, smcr, inshot, g...\n",
       "26                 [chick, get, fuck, hottest, nake, ladi]\n",
       "27       [happi, bday, luci, xoxo, love, beauti, pizza,...\n",
       "28       [haroldfriday, weekend, fill, sunbeam, everyon...\n",
       "29       [tri, noth, tri, know, love, rd, light, fave, ...\n",
       "                               ...                        \n",
       "17167                [peopl, anyth, fuck, attent, nowaday]\n",
       "17168    [creativ, bubbl, got, burst, look, forward, da...\n",
       "17169    [tomorrow, gon, na, big, day, go, deliv, first...\n",
       "17170                   [thank, babi, giggl, thank, posit]\n",
       "17171                  [model, love, u, take, u, time, ur]\n",
       "17172    [life, u, grow, learn, pple, work, fuck, u, co...\n",
       "17173    [storm, rain, togeth, destroy, town, becam, na...\n",
       "17174    [lovelgq, broken, ep, via, rnb, love, heabroke...\n",
       "17175    [spread, love, hate, prayingfororlando, lovean...\n",
       "17176                                  [racist, pay, ever]\n",
       "17177                         [thank, child, thank, posit]\n",
       "17178    [liverpool, walk, liverpool, starbuck, avidaeb...\n",
       "17179    [bakersfield, rooster, simul, want, climb, vas...\n",
       "17180    [por, sol, instagood, beauti, instadaili, inst...\n",
       "17181    [hell, yeah, great, surpris, present, enjoy, p...\n",
       "17182              [ur, joke, ur, defens, toward, everyth]\n",
       "17183    [enjoy, even, sun, bedroom, cozi, even, homesw...\n",
       "17184    [tonight, pm, gmt, special, earli, play, new, ...\n",
       "17185    [today, good, day, excercis, imreadi, sofucken...\n",
       "17186    [good, night, tea, music, billi, music, tea, m...\n",
       "17187    [love, life, createyourfutur, lifestyl, holida...\n",
       "17188    [black, professor, demon, propos, nazi, style,...\n",
       "17189    [learn, think, posit, posit, instagram, instag...\n",
       "17190    [love, pretti, happi, fresh, teenilici, fixder...\n",
       "17191    [damn, tuff, ruff, muff, techno, citi, ng, web...\n",
       "17192    [thought, factori, left, right, polaris, trump...\n",
       "17193    [feel, like, mermaid, hairflip, neverreadi, fo...\n",
       "17194    [hillari, campaign, today, ohio, omg, amp, use...\n",
       "17195    [happi, work, confer, right, mindset, lead, cu...\n",
       "17196    [song, glad, free, download, shoegaz, newmus, ...\n",
       "Name: tidy_tweet, Length: 17197, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweet_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tidy_tweet'] = tokenized_tweet\n",
    "df_test['tidy_tweet'] = tokenized_tweet_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exctracting feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector_size = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = gensim.models.Word2Vec(pd.concat([df['tidy_tweet'],df_test['tidy_tweet']]), min_count = 1,  \n",
    "                              size = feature_vector_size, window = 4,sg=1)  # Word2Vec trained on both train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vectors(row):\n",
    "    l= np.zeros((feature_vector_size,1))\n",
    " \n",
    "    for i in row:\n",
    "        \n",
    "        l = np.add ( l , model1.wv[i].reshape((feature_vector_size,1)) )\n",
    "    avg_vec= np.divide(l,len(row))\n",
    "      \n",
    "    \n",
    "    return avg_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = df['tidy_tweet'].apply(get_feature_vectors)\n",
    "test_features = df_test['tidy_tweet'].apply(get_feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962,)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack(train_features.values).reshape((-1, feature_vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.stack(test_features.values).reshape((-1, feature_vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17197, 250)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  506,   506,   506, ..., 31781, 31781, 31781]),\n",
       " array([  0,   1,   2, ..., 247, 248, 249]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.isnan(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.nan_to_num(X) # handling na values\n",
    "test_X = np.nan_to_num(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 250)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:25000,:] # sample train and validation sets\n",
    "X_val = X[25000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y[:25000]\n",
    "y_val = y[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_min= 23246\n",
    "ct_maj= 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE(ratio={0:23246,1:3400}, k_neighbors=1,random_state=0) # minority oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SMOTE(k=None, k_neighbors=1, kind='regular', m=None, m_neighbors=10, n_jobs=1,\n",
       "   out_step=0.5, random_state=0, ratio={0: 23246, 1: 3400},\n",
       "   svm_estimator=None)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resam,y_resam=  smt.fit_sample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf =RandomForestClassifier(random_state=0,n_estimators=25,max_features=50,class_weight={1:2242/31962,0:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xg_cf = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True,\n",
       "            class_weight={1: 0.07014579813528565, 0: 1}, criterion='gini',\n",
       "            max_depth=None, max_features=50, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=25, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_resam,y_resam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9495834530307383"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6340,  134],\n",
       "       [ 217,  271]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_val,rf.predict(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.606942889137738"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val,rf.predict(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26646, 250)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_resam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "923"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rf.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rf.predict(test_X)).to_csv('pred_twitter_smote_3k_stop_sg.csv') # write predictions to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
